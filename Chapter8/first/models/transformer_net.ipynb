{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 风格迁移网络\n",
    "主体网络TransformerNet由三个小网络组成,并且小网络内部的模块是单一的，分别是\n",
    "* 小网络\n",
    "* * 小网络的模块\n",
    "* downconv_layers\n",
    "* * 3*DownConv\n",
    "* residulablock_layers\n",
    "* * 5* ResidulaBlock\n",
    "* upconv_layers\n",
    "* * UpConv+UpConv+Conv\n",
    "\n",
    "在这章，所有的卷积层都变成了ReflectionPad2d+Conv2d，所以我们用一个Conv模块进行代替。 这一块还能继续优化，待续\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransfomerNet,self).__init__()\n",
    "        \n",
    "        # 下卷积\n",
    "        # H-->H-->H/2-->H/4\n",
    "        self.downconv_layer = nn.Sequential(\n",
    "            DownConv( 3, 32,kernel_size=9,stride=1),\n",
    "            DownConv(32, 64,kernel_size=3,stride=2), \n",
    "            DownConv(64,128,kernel_size=3,stride=2)  \n",
    "\n",
    "        )\n",
    "        \n",
    "        \n",
    "        #  Residual layers\n",
    "        # 这里应该可以改进，待续\n",
    "        self.residualblock_layers = nn.Sequential(\n",
    "            ResidualBlock(128,128,3,1),\n",
    "            ResidualBlock(128,128,3,1),\n",
    "            ResidualBlock(128,128,3,1),\n",
    "            ResidualBlock(128,128,3,1),\n",
    "            ResidualBlock(128,128,3,1)\n",
    "        )\n",
    "        \n",
    "        # Upsampling Layers\n",
    "        # 第八章的和第七章的从小变大的方法不一样，第七章的变大是通过逆卷积的方法ConvTranspose2d()，但这次是通过Unsampling的方法。\n",
    "        # \n",
    "        self.upconv_layer = nn.Sequential(\n",
    "            UpConv(128,64,3,1,upsample = 2),\n",
    "            UpConv(64,32,3,1,upsample=2),\n",
    "            UpConv(32,3,9,1)\n",
    "        )\n",
    "        # \n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.conv_layer(x)\n",
    "        x = self.residualblock_layers(x)\n",
    "        x = self.upconv_layer(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownConv(nn.Module):\n",
    "    \"\"\"\n",
    "    下卷积\n",
    "    使用边界反射补充，原因未知\n",
    "    当stride=2的时候，为什么还要边界反射补充\n",
    "    每一个下卷积层的block都是一样的 relected+InstanceNorm2d+relu\n",
    "    upsample上采样，是pooling的逆过程\n",
    "    \"\"\"\n",
    "    def __init__(self,in_channels, out_channels, kernel_size, stride):\n",
    "        super(DownConv,self).__init__()\n",
    "        self.main = nn.Sequential(   \n",
    "            Conv(in_channels,out_channels,kernel_size,stride),\n",
    "            nn.InstanceNorm2d(out_channels,affine=True),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.main(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidulaBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    残差层，用于过渡\n",
    "    这个残差层写的也不一样，第六章实现的block是conv+x->relu,但作者给出的代码是conv+x->输出\n",
    "    有待结果验证\n",
    "    RelU也没有设置True\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,input_channels,out_channels,kernels,stride):\n",
    "        super(ResidulaBlock,self).__init__()\n",
    "        self.left = nn.Sequential(\n",
    "            Conv(input_channels,out_channels,kernels,stride),\n",
    "            nn.InstanceNorm2d(out_channels,affine=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            Conv(out_channels,out_channels,kernels,stride),\n",
    "            nn.InstanceNorm2d(out_channels,affine=True)\n",
    "            \n",
    "        )\n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self,x):\n",
    "        out = self.left(x)\n",
    "        residual = x\n",
    "        out = out + x\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpConv(nn.Module):\n",
    "    \"\"\"\n",
    "    上卷积\n",
    "    这里使用边界反射填充\n",
    "    这里有疑问，不知道怎么实现的\n",
    "    ref:http://distill.pub/2016/deconv-checkerboard/\n",
    "    待续\n",
    "    upsample上采样，是pooling的逆过程，刚刚看了看说明，感觉不太对，还有一个align_corner也没有想明白\n",
    "    待续\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(UpConv,self).__init__(self,input_channels,out_channels,kernels,stride, upsample=None)\n",
    "        self.upsample = upsample\n",
    "        if self.upsample:\n",
    "            self.upsample_layer = nn.Upsample(scale_factor=upsample)\n",
    "        self.main = nn.Sequential(\n",
    "            Conv(input_channels,out_channels,kernels,stride),\n",
    "            nn.InstanceNorm2d(out_channels,affine=True),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def forward(self,x):\n",
    "        if self.upsample:\n",
    "            x = self.upsample_layer(x)\n",
    "        x = self.main(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv(nn.Module):\n",
    "    # 相当于在写ReflectionPad+Conv,因为所有的conv都变成了这个样子，所以我们专门拿出来，避免造轮子\n",
    "    # 这一块也可以重新优化，先根据作者的来，如果不行，再换。\n",
    "    def __init__(self,in_channels,out_channels,kernel_size,stride):\n",
    "        super(Conv,self).__init__()\n",
    "        self.main = nn.Sequential(   \n",
    "            nn.ReflectionPad2d(int(kernel_size/2)),\n",
    "            nn.Conv2d(in_channels,out_channels,kernel_size,stride)\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
