{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从第七章和第八章的作者的写作风格来看，作者开始把Config开始放在了main.py之内，而不再是重新放在一个.py文件内，我开始思考这样放在一起和分开放的区别，两者在getsorce（）方面没有任何区别，单纯地从代码上看，写在一起未尝不好，这样在调试的时候，直接打开train就可以看到配置文件，但是在属性输入方面就要差一些，没有opt.parse()来得爽快一些，而且作者开始停止了help函数的设置，我决定继续使用分开的方式，因为我感觉很条理，在后期修改其他配置的时候，是不需要打开train.py的，不需要担心会污染训练的代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/first\n",
      "importing Jupyter notebook from config.ipynb\n",
      "importing Jupyter notebook from /home/tjj/pytorchtest/chapter8/utils/visualize.ipynb\n",
      "importing Jupyter notebook from /home/tjj/pytorchtest/chapter8/utils/utils.ipynb\n",
      "importing Jupyter notebook from /home/tjj/pytorchtest/chapter8/models/transformer_net.ipynb\n",
      "importing Jupyter notebook from /home/tjj/pytorchtest/chapter8/models/PackedVGG.ipynb\n"
     ]
    }
   ],
   "source": [
    "import Ipynb_importer\n",
    "from config import opt \n",
    "from utils import Visualizer\n",
    "from models import TransformerNet, Vgg16\n",
    "from utils import get_style_data\n",
    "from utils import normalize_batch\n",
    "from utils import gram_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主文件：\n",
    "train(**kwargs):  \n",
    "val(model,dataloader):  \n",
    "test(**kwargs):   \n",
    "help():  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里新介绍一种作者用的怎么使用cuda的方法  \n",
    "device=t.device('cuda') if opt.use_gpu else t.device('cpu')  \n",
    "vgg.to(device)  \n",
    "代替了之前一直使用的很不方便的\n",
    "if opt.use_gpu:\n",
    "    model.cuda()\n",
    "    \n",
    "但是一直还没有使用 t.cuda.set_device(1)这个方法，\n",
    "刚刚查了下好像可以,torch.device('cuda')采用的是逻辑GPU，\n",
    "torch.cuda.current_device()用来查询当前使用的默认GPU\n",
    "我感觉这么写可以，无法验证\n",
    "torch.cuda.set_device(1)\n",
    "device = torch.device('cuda')\n",
    "models.to(device)\n",
    "但是这一种方法只是用单GPU方法，查看document，我们可以发现set_device并不好用，不如CUDA_VISIBLE_DEVICES好用，这个适用于多GPU，待补坑。\n",
    "\n",
    "还要一种方法是 \n",
    "with torch.cuda.device(1):   \n",
    "请注意一下torch.device和torch.cuda.device的区别\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch as t\n",
    "import torchvision as tv\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader \n",
    "from torchnet import meter\n",
    "from tqdm import tqdm\n",
    "# 只在jupyter中见过tqdm的显示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(**kwargs):\n",
    "    # step1:config\n",
    "    opt.parse(**kwargs)\n",
    "    vis = Visualizer(opt.env)\n",
    "    device = t.device('cuda') if opt.use_gpu else t.device('cpu')\n",
    "    \n",
    "    # step2:data\n",
    "    # dataloader, style_img\n",
    "    # 这次图片的处理和之前不一样，之前都是normalize,这次改成了lambda表达式乘以255，这种转化之后要给出一个合理的解释\n",
    "    # 图片共分为两种，一种是原图，一种是风格图片，在作者的代码里，原图用于训练，需要很多，风格图片需要一张，用于损失函数\n",
    "    \n",
    "    transforms = T.Compose(\n",
    "        T.Resize(opt.image_size),\n",
    "        T.CenterCrop(opt.image_size),\n",
    "        T.ToTensor(),\n",
    "        T.Lambda(lambda x: x*255)    \n",
    "    )\n",
    "    # 这次获取图片的方式和第七章一样，仍然是ImageFolder的方式，而不是dataset的方式\n",
    "    dataset = tv.datasets.ImageFolder(opt.data_root,transform=transforms)\n",
    "    dataloader = DataLoader(dataset,batch_size=opt.batch_size,shuffle=True,num_workers=opt.num_workers,drop_last=True)\n",
    "    \n",
    "    style_img = get_style_data(opt.style_path) # 1*c*H*W\n",
    "    style_img.to(device)\n",
    "    vis.img('style_image',(style_img.data[0]*0.225+0.45.clamp(min=0,max=1))) # 个人觉得这个没必要，下次可以实验一下\n",
    "    \n",
    "    # step3: model：Transformer_net 和 损失网络vgg16\n",
    "    # 整个模型分为两部分，一部分是转化模型TransformerNet，用于转化原始图片，一部分是损失模型Vgg16，用于评价损失函数，\n",
    "    # 在这里需要注意一下，Vgg16只是用于评价损失函数的，所以它的参数不参与反向传播，只有Transformer的参数参与反向传播，\n",
    "    # 也就意味着，我们只训练TransformerNet，只保存TransformerNet的参数，Vgg16的参数是在网络设计时就已经加载进去的。\n",
    "    # Vgg16是以验证model.eval()的方式在运行，表示其中涉及到pooling等层会发生改变\n",
    "    # 那模型什么时候开始model.eval()呢，之前是是val和test中就会这样设置，那么Vgg16的设置理由是什么？\n",
    "    # 这里加载模型的时候，作者使用了简单的map_location的记录方法，更轻巧一些\n",
    "    # 发现作者在写这些的时候越来越趋向方便的方式\n",
    "    # 在cuda的使用上，模型的cuda是直接使用的，而数据的cuda是在正式训练的时候才使用的，注意一下两者的区别\n",
    "    # 在第七章作者是通过两种方式实现网络分离的，一种是对于前面网络netg,进行 fake_img = netg(noises).detach(),使得非叶子节点变成一个类似不需要邱求导的叶子节点\n",
    "    # 第四章还需要重新看，\n",
    "    \n",
    "    transformer_net = TransformerNet()\n",
    "    \n",
    "    if opt.model_path:\n",
    "        transformer_net.load_state_dict(t.load(opt.model_path,map_location= lambda _s, _: _s))    \n",
    "    transformer_net.to(device)\n",
    "    \n",
    "\n",
    "    \n",
    "    # step3： criterion and optimizer\n",
    "    optimizer = t.optim.Adam(transformer_net.parameters(),opt.lr)\n",
    "    # 此通过vgg16实现的，损失函数包含两个Gram矩阵和均方误差，所以，此外，我们还需要求Gram矩阵和均方误差\n",
    "    vgg16 = Vgg16().eval() # 待验证\n",
    "    vgg16.to(device)\n",
    "    # vgg的参数不需要倒数，但仍然需要反向传播\n",
    "    # 回头重新考虑一下detach和requires_grad的区别\n",
    "    for param in vgg16.parameters():\n",
    "        param.requires_grad = False\n",
    "    criterion = t.nn.MSELoss(reduce=True, size_average=True)\n",
    "    \n",
    "    \n",
    "    # step4: meter 损失统计\n",
    "    style_meter = meter.AverageValueMeter()\n",
    "    content_meter = meter.AverageValueMeter()\n",
    "    total_meter = meter.AverageValueMeter()\n",
    "    \n",
    "    # step5.2：loss 补充\n",
    "    # 求style_image的gram矩阵\n",
    "    # gram_style:list [relu1_2,relu2_2,relu3_3,relu4_3] 每一个是b*c*c大小的tensor\n",
    "    with t.no_grad():\n",
    "        features = vgg16(style_img)\n",
    "        gram_style = [gram_matrix(feature) for feature in features]\n",
    "    \n",
    "    # step5： train\n",
    "    for epoch in range(opt.epoches):\n",
    "        style_meter.reset()\n",
    "        content_meter.reset()\n",
    "        \n",
    "        # step5.1: train\n",
    "        for ii,(data,_) in tqdm(enumerate(dataloader)):\n",
    "            # 这里作者没有进行 Variable(),与之前不同\n",
    "            # pytorch 0.4.之后tensor和Variable不再严格区分，创建的tensor就是variable\n",
    "            # https://mp.weixin.qq.com/s?__biz=MzI0ODcxODk5OA==&mid=2247494701&idx=2&sn=ea8411d66038f172a2f553770adccbec&chksm=e99edfd4dee956c23c47c7bb97a31ee816eb3a0404466c1a57c12948d807c975053e38b18097&scene=21#wechat_redirect\n",
    "            data = data.to(device)\n",
    "            y = transformer_net(data)\n",
    "            \n",
    "            # vgg对输入的图片需要进行归一化\n",
    "            data = normalize_batch(data)\n",
    "            y = normalize_batch(y)\n",
    "            feature_data = vgg(data)\n",
    "            feature_y = vgg(y) \n",
    "            # 疑问？？现在的feature是一个什么样子的向量？\n",
    "            \n",
    "            # step5.2: loss:content loss and style loss\n",
    "            # content_loss\n",
    "            # 在这里和书上的讲的不一样，书上是relu3_3,代码用的是relu2_2\n",
    "            # https://blog.csdn.net/zhangxb35/article/details/72464152?utm_source=itdadao&utm_medium=referral\n",
    "            # 均方误差指的是一个像素点的损失，可以理解N*b*h*w个元素加起来，然后除以N*b*h*w\n",
    "            # 随机梯度下降法本身就是对batch内loss求平均后反向传播\n",
    "            content_loss = opt.content_weight*criterion(feature_data.relu2_2,feature_y.relu2_2)\n",
    "            # style loss\n",
    "            # style loss:relu1_2,relu2_2,relu3_3,relu3_4 \n",
    "            # 此时需要求每一张图片的gram矩阵\n",
    "            \n",
    "            style_loss = 0\n",
    "            # tensor也可以 for i in tensor:,此时只拆解外面一层的tensor\n",
    "            # ft_y:b*c*h*w, gm_s:1*c*h*w\n",
    "            for ft_y, gm_s in zip(feature_y, gram_style):\n",
    "                gram_y = gram_matrix(ft_y)\n",
    "                style_loss += criterion(gram_y, gm_s.expand_as(gram_y))\n",
    "            style_loss *= opt.style_weight\n",
    "            \n",
    "            total_loss = content_loss + style_loss\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            # 获取tensor的值 tensor.item()   tensor.tolist()\n",
    "            content_meter.add(content_loss.item())\n",
    "            style_meter.add(style_loss.item())\n",
    "            total_meter.add(total_loss.item())\n",
    "            \n",
    "            # step5.3: visualize\n",
    "            if (ii+1)%opt.print_freq == 0 and opt.vis:\n",
    "                # 为什么总是以这种形式进行debug\n",
    "                if os.path.exits(opt.debug_file):\n",
    "                    import ipdb\n",
    "                    ipdb.set_trace()\n",
    "                vis.plot('content_loss',content_meter.value()[0])\n",
    "                vis.plot('style_loss',style_meter.value()[0])\n",
    "                vis.plot('total_loss',total_meter.value()[0])\n",
    "                # 因为现在data和y都已经经过了normalize，变成了-2~2，所以需要把它变回去0-1\n",
    "                vis.img('input',(data.data*0.225+0.45)[0].clamp(min=0,max=1))\n",
    "                vis.img('output',(y.data*0.225+0.45)[0].clamp(min=0,max=1))\n",
    "            \n",
    "        # step 5.4 save and validate and visualize\n",
    "        if (epoch+1) % opt.save_every == 0:\n",
    "            t.save(transformer.state_dict(), 'checkpoints/%s_style.pth' % epoch)\n",
    "            # 保存图片的几种方法，第七章的是 \n",
    "            # tv.utils.save_image(fix_fake_imgs,'%s/%s.png' % (opt.img_save_path, epoch),normalize=True, range=(-1,1))\n",
    "            # vis.save竟然没找到  我的神   \n",
    "            vis.save([opt.env])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@t.no_grad()\n",
    "def stylize(**kwargs):\n",
    "    opt.parse(**kwargs)\n",
    "    device = t.device('cuda') if opt.use_gpu else t.device('cpu')\n",
    "    # data\n",
    "    # 对单张图片进行加载验证\n",
    "    content_image = tv.datasets.folder.default_loader(opt.content_path)\n",
    "    # 这个transform更看不懂了，竟然不需要resize和centrocrop，这图片大小不一致，能送进去么？？？？\n",
    "    content_transform = tv.transforms.Compose([\n",
    "        tv.transforms.ToTensor(),\n",
    "        tv.transforms.Lambda(lambda x: x.mul(255))\n",
    "    ])\n",
    "    \n",
    "    content_image = content_transform(content_image)\n",
    "    content_image = content_image.unsqueeze(0).to(device).detach()\n",
    "    \n",
    "    # model\n",
    "    style_model = TransformerNet().eval()\n",
    "    style_model.load_state_dict(t.load(opt.model_path, map_location=lambda _s, _: _s))\n",
    "    style.to(device)\n",
    "    \n",
    "    # stylize\n",
    "    output = style_model(content_image) # 0-255~\n",
    "    output_data = (output.cpu().data[0]/255).clamp(min=0, max=1)\n",
    "    tv.utils.save_image(output_data,opt.result_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def help():\n",
    "    print(\"\"\"\n",
    "    usage: print file.py <function> [--args=value]\n",
    "    <function>: train | stylize\n",
    "    example:\n",
    "        python {0} train\n",
    "        python {0} stylize\n",
    "        python {0} help\n",
    "    avaiable args :\n",
    "    \"\"\".format(__file__))\n",
    "    \n",
    "    from inspect import getsource\n",
    "    source = getsource(opt.__class__)\n",
    "    print(source)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fire trace:\n",
      "1. Initial component\n",
      "2. ('Cannot find target in dict:', '-f', {'__name__': '__main__', '__doc__': 'Automatically created module for IPython interactive environment', '__package__': None, '__loader__': None, '__spec__': None, '__builtin__': <module 'builtins' (built-in)>, '__builtins__': <module 'builtins' (built-in)>, '_ih': ['', 'import Ipynb_importer\\nfrom config import opt \\nfrom utils import Visualizer\\nfrom models import TransformerNet, Vgg16\\nfrom utils import get_style_data\\nfrom utils import normalize_batch\\nfrom utils import gram_matrix', 'from torch import nn\\nimport torch as t\\nimport torchvision as tv\\nimport torchvision.transforms as T\\nfrom torch.utils.data import DataLoader \\nfrom torchnet import meter\\nfrom tqdm import tqdm\\n# 只在jupyter中见过tqdm的显示', \"def train(**kwargs):\\n    # step1:config\\n    opt.parse(**kwargs)\\n    vis = Visualizer(opt.env)\\n    device = t.device('cuda') if opt.use_gpu else t.device('cpu')\\n    \\n    # step2:data\\n    # dataloader, style_img\\n    # 这次图片的处理和之前不一样，之前都是normalize,这次改成了lambda表达式乘以255，这种转化之后要给出一个合理的解释\\n    # 图片共分为两种，一种是原图，一种是风格图片，在作者的代码里，原图用于训练，需要很多，风格图片需要一张，用于损失函数\\n    \\n    transforms = T.Compose(\\n        T.Resize(opt.image_size),\\n        T.CenterCrop(opt.image_size),\\n        T.ToTensor(),\\n        T.Lambda(lambda x: x*255)    \\n    )\\n    # 这次获取图片的方式和第七章一样，仍然是ImageFolder的方式，而不是dataset的方式\\n    dataset = tv.datasets.ImageFolder(opt.data_root,transform=transforms)\\n    dataloader = DataLoader(dataset,batch_size=opt.batch_size,shuffle=True,num_workers=opt.num_workers,drop_last=True)\\n    \\n    style_img = get_style_data(opt.style_path) # 1*c*H*W\\n    style_img.to(device)\\n    vis.img('style_image',(style_img.data[0]*0.225+0.45.clamp(min=0,max=1))) # 个人觉得这个没必要，下次可以实验一下\\n    \\n    # step3: model：Transformer_net 和 损失网络vgg16\\n    # 整个模型分为两部分，一部分是转化模型TransformerNet，用于转化原始图片，一部分是损失模型Vgg16，用于评价损失函数，\\n    # 在这里需要注意一下，Vgg16只是用于评价损失函数的，所以它的参数不参与反向传播，只有Transformer的参数参与反向传播，\\n    # 也就意味着，我们只训练TransformerNet，只保存TransformerNet的参数，Vgg16的参数是在网络设计时就已经加载进去的。\\n    # Vgg16是以验证model.eval()的方式在运行，表示其中涉及到pooling等层会发生改变\\n    # 那模型什么时候开始model.eval()呢，之前是是val和test中就会这样设置，那么Vgg16的设置理由是什么？\\n    # 这里加载模型的时候，作者使用了简单的map_location的记录方法，更轻巧一些\\n    # 发现作者在写这些的时候越来越趋向方便的方式\\n    # 在cuda的使用上，模型的cuda是直接使用的，而数据的cuda是在正式训练的时候才使用的，注意一下两者的区别\\n    # 在第七章作者是通过两种方式实现网络分离的，一种是对于前面网络netg,进行 fake_img = netg(noises).detach(),使得非叶子节点变成一个类似不需要邱求导的叶子节点\\n    # 第四章还需要重新看，\\n    \\n    transformer_net = TransformerNet()\\n    \\n    if opt.model_path:\\n        transformer_net.load_state_dict(t.load(opt.model_path,map_location= lambda _s, _: _s))    \\n    transformer_net.to(device)\\n    \\n\\n    \\n    # step3： criterion and optimizer\\n    optimizer = t.optim.Adam(transformer_net.parameters(),opt.lr)\\n    # 此通过vgg16实现的，损失函数包含两个Gram矩阵和均方误差，所以，此外，我们还需要求Gram矩阵和均方误差\\n    vgg16 = Vgg16().eval() # 待验证\\n    vgg16.to(device)\\n    # vgg的参数不需要倒数，但仍然需要反向传播\\n    # 回头重新考虑一下detach和requires_grad的区别\\n    for param in vgg16.parameters():\\n        param.requires_grad = False\\n    criterion = t.nn.MSELoss(reduce=True, size_average=True)\\n    \\n    \\n    # step4: meter 损失统计\\n    style_meter = meter.AverageValueMeter()\\n    content_meter = meter.AverageValueMeter()\\n    total_meter = meter.AverageValueMeter()\\n    \\n    # step5.2：loss 补充\\n    # 求style_image的gram矩阵\\n    # gram_style:list [relu1_2,relu2_2,relu3_3,relu4_3] 每一个是b*c*c大小的tensor\\n    with t.no_grad():\\n        features = vgg16(style_img)\\n        gram_style = [gram_matrix(feature) for feature in features]\\n    \\n    # step5： train\\n    for epoch in range(opt.epoches):\\n        style_meter.reset()\\n        content_meter.reset()\\n        \\n        # step5.1: train\\n        for ii,(data,_) in tqdm(enumerate(dataloader)):\\n            # 这里作者没有进行 Variable(),与之前不同\\n            # pytorch 0.4.之后tensor和Variable不再严格区分，创建的tensor就是variable\\n            # https://mp.weixin.qq.com/s?__biz=MzI0ODcxODk5OA==&mid=2247494701&idx=2&sn=ea8411d66038f172a2f553770adccbec&chksm=e99edfd4dee956c23c47c7bb97a31ee816eb3a0404466c1a57c12948d807c975053e38b18097&scene=21#wechat_redirect\\n            data = data.to(device)\\n            y = transformer_net(data)\\n            \\n            # vgg对输入的图片需要进行归一化\\n            data = normalize_batch(data)\\n            y = normalize_batch(y)\\n            feature_data = vgg(data)\\n            feature_y = vgg(y) \\n            # 疑问？？现在的feature是一个什么样子的向量？\\n            \\n            # step5.2: loss:content loss and style loss\\n            # content_loss\\n            # 在这里和书上的讲的不一样，书上是relu3_3,代码用的是relu2_2\\n            # https://blog.csdn.net/zhangxb35/article/details/72464152?utm_source=itdadao&utm_medium=referral\\n            # 均方误差指的是一个像素点的损失，可以理解N*b*h*w个元素加起来，然后除以N*b*h*w\\n            # 随机梯度下降法本身就是对batch内loss求平均后反向传播\\n            content_loss = opt.content_weight*criterion(feature_data.relu2_2,feature_y.relu2_2)\\n            # style loss\\n            # style loss:relu1_2,relu2_2,relu3_3,relu3_4 \\n            # 此时需要求每一张图片的gram矩阵\\n            \\n            style_loss = 0\\n            # tensor也可以 for i in tensor:,此时只拆解外面一层的tensor\\n            # ft_y:b*c*h*w, gm_s:1*c*h*w\\n            for ft_y, gm_s in zip(feature_y, gram_style):\\n                gram_y = gram_matrix(ft_y)\\n                style_loss += criterion(gram_y, gm_s.expand_as(gram_y))\\n            style_loss *= opt.style_weight\\n            \\n            total_loss = content_loss + style_loss\\n            optimizer.zero_grad()\\n            total_loss.backward()\\n            optimizer.step()\\n            # 获取tensor的值 tensor.item()   tensor.tolist()\\n            content_meter.add(content_loss.item())\\n            style_meter.add(style_loss.item())\\n            total_meter.add(total_loss.item())\\n            \\n            # step5.3: visualize\\n            if (ii+1)%opt.print_freq == 0 and opt.vis:\\n                # 为什么总是以这种形式进行debug\\n                if os.path.exits(opt.debug_file):\\n                    import ipdb\\n                    ipdb.set_trace()\\n                vis.plot('content_loss',content_meter.value()[0])\\n                vis.plot('style_loss',style_meter.value()[0])\\n                vis.plot('total_loss',total_meter.value()[0])\\n                # 因为现在data和y都已经经过了normalize，变成了-2~2，所以需要把它变回去0-1\\n                vis.img('input',(data.data*0.225+0.45)[0].clamp(min=0,max=1))\\n                vis.img('output',(y.data*0.225+0.45)[0].clamp(min=0,max=1))\\n            \\n        # step 5.4 save and validate and visualize\\n        if (epoch+1) % opt.save_every == 0:\\n            t.save(transformer.state_dict(), 'checkpoints/%s_style.pth' % epoch)\\n            # 保存图片的几种方法，第七章的是 \\n            # tv.utils.save_image(fix_fake_imgs,'%s/%s.png' % (opt.img_save_path, epoch),normalize=True, range=(-1,1))\\n            # vis.save竟然没找到  我的神   \\n            vis.save([opt.env])\", \"def stylize(**kwargs):\\n    opt.parse(**kwargs)\\n    device = t.device('cuda') if opt.use_gpu else t.device('cpu')\\n    # data\\n    # 对单张图片进行加载验证\\n    content_image = tv.datasets.folder.default_loader(opt.content_path)\\n    # 这个transform更看不懂了，竟然不需要resize和centrocrop，这图片大小不一致，能送进去么？？？？\\n    content_transform = tv.transforms.Compose([\\n        tv.transforms.ToTensor(),\\n        tv.transforms.Lambda(lambda x: x.mul(255))\\n    ])\\n    \\n    content_image = content_transform(content_image)\\n    content_image = content_image.unsqueeze(0).to(device).detach()\\n    \\n    # model\\n    style_model = TransformerNet().eval()\\n    style_model.load_state_dict(t.load(opt.model_path, map_location=lambda _s, _: _s))\\n    style.to(device)\\n    \\n    # stylize\\n    output = style_model(content_image) # 0-255~\\n    output_data = (output.cpu().data[0]/255).clamp(min=0, max=1)\\n    tv.utils.save_image(output_data,opt.result_path)\", 'def help():\\n    print(\"\"\"\\n    usage: print file.py <function> [--args=value]\\n    <function>: train | stylize\\n    example:\\n        python {0} train\\n        python {0} stylize\\n        python {0} help\\n    avaiable args :\\n    \"\"\".format(__file__))\\n    \\n    from inspect import getsource\\n    source = getsource(opt.__class__)\\n    \\n    ', \"def __name__ == '__main__'():\\n    import fire\\n    fire.Fire()\\n    \", \"if __name__ == '__main__'():\\n    import fire\\n    fire.Fire()\\n    \", \"if __name__ == '__main__':\\n    import fire\\n    fire.Fire()\\n    \"], '_oh': {}, '_dh': ['/home/tjj/pytorchtest/chapter8'], 'In': ['', 'import Ipynb_importer\\nfrom config import opt \\nfrom utils import Visualizer\\nfrom models import TransformerNet, Vgg16\\nfrom utils import get_style_data\\nfrom utils import normalize_batch\\nfrom utils import gram_matrix', 'from torch import nn\\nimport torch as t\\nimport torchvision as tv\\nimport torchvision.transforms as T\\nfrom torch.utils.data import DataLoader \\nfrom torchnet import meter\\nfrom tqdm import tqdm\\n# 只在jupyter中见过tqdm的显示', \"def train(**kwargs):\\n    # step1:config\\n    opt.parse(**kwargs)\\n    vis = Visualizer(opt.env)\\n    device = t.device('cuda') if opt.use_gpu else t.device('cpu')\\n    \\n    # step2:data\\n    # dataloader, style_img\\n    # 这次图片的处理和之前不一样，之前都是normalize,这次改成了lambda表达式乘以255，这种转化之后要给出一个合理的解释\\n    # 图片共分为两种，一种是原图，一种是风格图片，在作者的代码里，原图用于训练，需要很多，风格图片需要一张，用于损失函数\\n    \\n    transforms = T.Compose(\\n        T.Resize(opt.image_size),\\n        T.CenterCrop(opt.image_size),\\n        T.ToTensor(),\\n        T.Lambda(lambda x: x*255)    \\n    )\\n    # 这次获取图片的方式和第七章一样，仍然是ImageFolder的方式，而不是dataset的方式\\n    dataset = tv.datasets.ImageFolder(opt.data_root,transform=transforms)\\n    dataloader = DataLoader(dataset,batch_size=opt.batch_size,shuffle=True,num_workers=opt.num_workers,drop_last=True)\\n    \\n    style_img = get_style_data(opt.style_path) # 1*c*H*W\\n    style_img.to(device)\\n    vis.img('style_image',(style_img.data[0]*0.225+0.45.clamp(min=0,max=1))) # 个人觉得这个没必要，下次可以实验一下\\n    \\n    # step3: model：Transformer_net 和 损失网络vgg16\\n    # 整个模型分为两部分，一部分是转化模型TransformerNet，用于转化原始图片，一部分是损失模型Vgg16，用于评价损失函数，\\n    # 在这里需要注意一下，Vgg16只是用于评价损失函数的，所以它的参数不参与反向传播，只有Transformer的参数参与反向传播，\\n    # 也就意味着，我们只训练TransformerNet，只保存TransformerNet的参数，Vgg16的参数是在网络设计时就已经加载进去的。\\n    # Vgg16是以验证model.eval()的方式在运行，表示其中涉及到pooling等层会发生改变\\n    # 那模型什么时候开始model.eval()呢，之前是是val和test中就会这样设置，那么Vgg16的设置理由是什么？\\n    # 这里加载模型的时候，作者使用了简单的map_location的记录方法，更轻巧一些\\n    # 发现作者在写这些的时候越来越趋向方便的方式\\n    # 在cuda的使用上，模型的cuda是直接使用的，而数据的cuda是在正式训练的时候才使用的，注意一下两者的区别\\n    # 在第七章作者是通过两种方式实现网络分离的，一种是对于前面网络netg,进行 fake_img = netg(noises).detach(),使得非叶子节点变成一个类似不需要邱求导的叶子节点\\n    # 第四章还需要重新看，\\n    \\n    transformer_net = TransformerNet()\\n    \\n    if opt.model_path:\\n        transformer_net.load_state_dict(t.load(opt.model_path,map_location= lambda _s, _: _s))    \\n    transformer_net.to(device)\\n    \\n\\n    \\n    # step3： criterion and optimizer\\n    optimizer = t.optim.Adam(transformer_net.parameters(),opt.lr)\\n    # 此通过vgg16实现的，损失函数包含两个Gram矩阵和均方误差，所以，此外，我们还需要求Gram矩阵和均方误差\\n    vgg16 = Vgg16().eval() # 待验证\\n    vgg16.to(device)\\n    # vgg的参数不需要倒数，但仍然需要反向传播\\n    # 回头重新考虑一下detach和requires_grad的区别\\n    for param in vgg16.parameters():\\n        param.requires_grad = False\\n    criterion = t.nn.MSELoss(reduce=True, size_average=True)\\n    \\n    \\n    # step4: meter 损失统计\\n    style_meter = meter.AverageValueMeter()\\n    content_meter = meter.AverageValueMeter()\\n    total_meter = meter.AverageValueMeter()\\n    \\n    # step5.2：loss 补充\\n    # 求style_image的gram矩阵\\n    # gram_style:list [relu1_2,relu2_2,relu3_3,relu4_3] 每一个是b*c*c大小的tensor\\n    with t.no_grad():\\n        features = vgg16(style_img)\\n        gram_style = [gram_matrix(feature) for feature in features]\\n    \\n    # step5： train\\n    for epoch in range(opt.epoches):\\n        style_meter.reset()\\n        content_meter.reset()\\n        \\n        # step5.1: train\\n        for ii,(data,_) in tqdm(enumerate(dataloader)):\\n            # 这里作者没有进行 Variable(),与之前不同\\n            # pytorch 0.4.之后tensor和Variable不再严格区分，创建的tensor就是variable\\n            # https://mp.weixin.qq.com/s?__biz=MzI0ODcxODk5OA==&mid=2247494701&idx=2&sn=ea8411d66038f172a2f553770adccbec&chksm=e99edfd4dee956c23c47c7bb97a31ee816eb3a0404466c1a57c12948d807c975053e38b18097&scene=21#wechat_redirect\\n            data = data.to(device)\\n            y = transformer_net(data)\\n            \\n            # vgg对输入的图片需要进行归一化\\n            data = normalize_batch(data)\\n            y = normalize_batch(y)\\n            feature_data = vgg(data)\\n            feature_y = vgg(y) \\n            # 疑问？？现在的feature是一个什么样子的向量？\\n            \\n            # step5.2: loss:content loss and style loss\\n            # content_loss\\n            # 在这里和书上的讲的不一样，书上是relu3_3,代码用的是relu2_2\\n            # https://blog.csdn.net/zhangxb35/article/details/72464152?utm_source=itdadao&utm_medium=referral\\n            # 均方误差指的是一个像素点的损失，可以理解N*b*h*w个元素加起来，然后除以N*b*h*w\\n            # 随机梯度下降法本身就是对batch内loss求平均后反向传播\\n            content_loss = opt.content_weight*criterion(feature_data.relu2_2,feature_y.relu2_2)\\n            # style loss\\n            # style loss:relu1_2,relu2_2,relu3_3,relu3_4 \\n            # 此时需要求每一张图片的gram矩阵\\n            \\n            style_loss = 0\\n            # tensor也可以 for i in tensor:,此时只拆解外面一层的tensor\\n            # ft_y:b*c*h*w, gm_s:1*c*h*w\\n            for ft_y, gm_s in zip(feature_y, gram_style):\\n                gram_y = gram_matrix(ft_y)\\n                style_loss += criterion(gram_y, gm_s.expand_as(gram_y))\\n            style_loss *= opt.style_weight\\n            \\n            total_loss = content_loss + style_loss\\n            optimizer.zero_grad()\\n            total_loss.backward()\\n            optimizer.step()\\n            # 获取tensor的值 tensor.item()   tensor.tolist()\\n            content_meter.add(content_loss.item())\\n            style_meter.add(style_loss.item())\\n            total_meter.add(total_loss.item())\\n            \\n            # step5.3: visualize\\n            if (ii+1)%opt.print_freq == 0 and opt.vis:\\n                # 为什么总是以这种形式进行debug\\n                if os.path.exits(opt.debug_file):\\n                    import ipdb\\n                    ipdb.set_trace()\\n                vis.plot('content_loss',content_meter.value()[0])\\n                vis.plot('style_loss',style_meter.value()[0])\\n                vis.plot('total_loss',total_meter.value()[0])\\n                # 因为现在data和y都已经经过了normalize，变成了-2~2，所以需要把它变回去0-1\\n                vis.img('input',(data.data*0.225+0.45)[0].clamp(min=0,max=1))\\n                vis.img('output',(y.data*0.225+0.45)[0].clamp(min=0,max=1))\\n            \\n        # step 5.4 save and validate and visualize\\n        if (epoch+1) % opt.save_every == 0:\\n            t.save(transformer.state_dict(), 'checkpoints/%s_style.pth' % epoch)\\n            # 保存图片的几种方法，第七章的是 \\n            # tv.utils.save_image(fix_fake_imgs,'%s/%s.png' % (opt.img_save_path, epoch),normalize=True, range=(-1,1))\\n            # vis.save竟然没找到  我的神   \\n            vis.save([opt.env])\", \"def stylize(**kwargs):\\n    opt.parse(**kwargs)\\n    device = t.device('cuda') if opt.use_gpu else t.device('cpu')\\n    # data\\n    # 对单张图片进行加载验证\\n    content_image = tv.datasets.folder.default_loader(opt.content_path)\\n    # 这个transform更看不懂了，竟然不需要resize和centrocrop，这图片大小不一致，能送进去么？？？？\\n    content_transform = tv.transforms.Compose([\\n        tv.transforms.ToTensor(),\\n        tv.transforms.Lambda(lambda x: x.mul(255))\\n    ])\\n    \\n    content_image = content_transform(content_image)\\n    content_image = content_image.unsqueeze(0).to(device).detach()\\n    \\n    # model\\n    style_model = TransformerNet().eval()\\n    style_model.load_state_dict(t.load(opt.model_path, map_location=lambda _s, _: _s))\\n    style.to(device)\\n    \\n    # stylize\\n    output = style_model(content_image) # 0-255~\\n    output_data = (output.cpu().data[0]/255).clamp(min=0, max=1)\\n    tv.utils.save_image(output_data,opt.result_path)\", 'def help():\\n    print(\"\"\"\\n    usage: print file.py <function> [--args=value]\\n    <function>: train | stylize\\n    example:\\n        python {0} train\\n        python {0} stylize\\n        python {0} help\\n    avaiable args :\\n    \"\"\".format(__file__))\\n    \\n    from inspect import getsource\\n    source = getsource(opt.__class__)\\n    \\n    ', \"def __name__ == '__main__'():\\n    import fire\\n    fire.Fire()\\n    \", \"if __name__ == '__main__'():\\n    import fire\\n    fire.Fire()\\n    \", \"if __name__ == '__main__':\\n    import fire\\n    fire.Fire()\\n    \"], 'Out': {}, 'get_ipython': <bound method InteractiveShell.get_ipython of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7f0559033fd0>>, 'exit': <IPython.core.autocall.ZMQExitAutocall object at 0x7f054eef6fd0>, 'quit': <IPython.core.autocall.ZMQExitAutocall object at 0x7f054eef6fd0>, '_': '', '__': '', '___': '', '_i': \"if __name__ == '__main__'():\\n    import fire\\n    fire.Fire()\\n    \", '_ii': \"def __name__ == '__main__'():\\n    import fire\\n    fire.Fire()\\n    \", '_iii': 'def help():\\n    print(\"\"\"\\n    usage: print file.py <function> [--args=value]\\n    <function>: train | stylize\\n    example:\\n        python {0} train\\n        python {0} stylize\\n        python {0} help\\n    avaiable args :\\n    \"\"\".format(__file__))\\n    \\n    from inspect import getsource\\n    source = getsource(opt.__class__)\\n    \\n    ', '_i1': 'import Ipynb_importer\\nfrom config import opt \\nfrom utils import Visualizer\\nfrom models import TransformerNet, Vgg16\\nfrom utils import get_style_data\\nfrom utils import normalize_batch\\nfrom utils import gram_matrix', 'Ipynb_importer': <module 'Ipynb_importer' from '/home/tjj/pytorchtest/chapter8/Ipynb_importer.py'>, 'opt': <config.DefaultConfig object at 0x7f054f249390>, 'Visualizer': <class 'utils.visualize.Visualizer'>, 'TransformerNet': <class 'models.transformer_net.TransformerNet'>, 'Vgg16': <class 'models.PackedVGG.Vgg16'>, 'get_style_data': <function get_style_data at 0x7f054f242d90>, 'normalize_batch': <function normalize_batch at 0x7f04fa06bbf8>, 'gram_matrix': <function gram_matrix at 0x7f04f9f13950>, '_i2': 'from torch import nn\\nimport torch as t\\nimport torchvision as tv\\nimport torchvision.transforms as T\\nfrom torch.utils.data import DataLoader \\nfrom torchnet import meter\\nfrom tqdm import tqdm\\n# 只在jupyter中见过tqdm的显示', 'nn': <module 'torch.nn' from '/home/tjj/anaconda3/lib/python3.6/site-packages/torch/nn/__init__.py'>, 't': <module 'torch' from '/home/tjj/anaconda3/lib/python3.6/site-packages/torch/__init__.py'>, 'tv': <module 'torchvision' from '/home/tjj/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/__init__.py'>, 'T': <module 'torchvision.transforms' from '/home/tjj/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/transforms/__init__.py'>, 'DataLoader': <class 'torch.utils.data.dataloader.DataLoader'>, 'meter': <module 'torchnet.meter' from '/home/tjj/anaconda3/lib/python3.6/site-packages/torchnet/meter/__init__.py'>, 'tqdm': <class 'tqdm._tqdm.tqdm'>, '_i3': \"def train(**kwargs):\\n    # step1:config\\n    opt.parse(**kwargs)\\n    vis = Visualizer(opt.env)\\n    device = t.device('cuda') if opt.use_gpu else t.device('cpu')\\n    \\n    # step2:data\\n    # dataloader, style_img\\n    # 这次图片的处理和之前不一样，之前都是normalize,这次改成了lambda表达式乘以255，这种转化之后要给出一个合理的解释\\n    # 图片共分为两种，一种是原图，一种是风格图片，在作者的代码里，原图用于训练，需要很多，风格图片需要一张，用于损失函数\\n    \\n    transforms = T.Compose(\\n        T.Resize(opt.image_size),\\n        T.CenterCrop(opt.image_size),\\n        T.ToTensor(),\\n        T.Lambda(lambda x: x*255)    \\n    )\\n    # 这次获取图片的方式和第七章一样，仍然是ImageFolder的方式，而不是dataset的方式\\n    dataset = tv.datasets.ImageFolder(opt.data_root,transform=transforms)\\n    dataloader = DataLoader(dataset,batch_size=opt.batch_size,shuffle=True,num_workers=opt.num_workers,drop_last=True)\\n    \\n    style_img = get_style_data(opt.style_path) # 1*c*H*W\\n    style_img.to(device)\\n    vis.img('style_image',(style_img.data[0]*0.225+0.45.clamp(min=0,max=1))) # 个人觉得这个没必要，下次可以实验一下\\n    \\n    # step3: model：Transformer_net 和 损失网络vgg16\\n    # 整个模型分为两部分，一部分是转化模型TransformerNet，用于转化原始图片，一部分是损失模型Vgg16，用于评价损失函数，\\n    # 在这里需要注意一下，Vgg16只是用于评价损失函数的，所以它的参数不参与反向传播，只有Transformer的参数参与反向传播，\\n    # 也就意味着，我们只训练TransformerNet，只保存TransformerNet的参数，Vgg16的参数是在网络设计时就已经加载进去的。\\n    # Vgg16是以验证model.eval()的方式在运行，表示其中涉及到pooling等层会发生改变\\n    # 那模型什么时候开始model.eval()呢，之前是是val和test中就会这样设置，那么Vgg16的设置理由是什么？\\n    # 这里加载模型的时候，作者使用了简单的map_location的记录方法，更轻巧一些\\n    # 发现作者在写这些的时候越来越趋向方便的方式\\n    # 在cuda的使用上，模型的cuda是直接使用的，而数据的cuda是在正式训练的时候才使用的，注意一下两者的区别\\n    # 在第七章作者是通过两种方式实现网络分离的，一种是对于前面网络netg,进行 fake_img = netg(noises).detach(),使得非叶子节点变成一个类似不需要邱求导的叶子节点\\n    # 第四章还需要重新看，\\n    \\n    transformer_net = TransformerNet()\\n    \\n    if opt.model_path:\\n        transformer_net.load_state_dict(t.load(opt.model_path,map_location= lambda _s, _: _s))    \\n    transformer_net.to(device)\\n    \\n\\n    \\n    # step3： criterion and optimizer\\n    optimizer = t.optim.Adam(transformer_net.parameters(),opt.lr)\\n    # 此通过vgg16实现的，损失函数包含两个Gram矩阵和均方误差，所以，此外，我们还需要求Gram矩阵和均方误差\\n    vgg16 = Vgg16().eval() # 待验证\\n    vgg16.to(device)\\n    # vgg的参数不需要倒数，但仍然需要反向传播\\n    # 回头重新考虑一下detach和requires_grad的区别\\n    for param in vgg16.parameters():\\n        param.requires_grad = False\\n    criterion = t.nn.MSELoss(reduce=True, size_average=True)\\n    \\n    \\n    # step4: meter 损失统计\\n    style_meter = meter.AverageValueMeter()\\n    content_meter = meter.AverageValueMeter()\\n    total_meter = meter.AverageValueMeter()\\n    \\n    # step5.2：loss 补充\\n    # 求style_image的gram矩阵\\n    # gram_style:list [relu1_2,relu2_2,relu3_3,relu4_3] 每一个是b*c*c大小的tensor\\n    with t.no_grad():\\n        features = vgg16(style_img)\\n        gram_style = [gram_matrix(feature) for feature in features]\\n    \\n    # step5： train\\n    for epoch in range(opt.epoches):\\n        style_meter.reset()\\n        content_meter.reset()\\n        \\n        # step5.1: train\\n        for ii,(data,_) in tqdm(enumerate(dataloader)):\\n            # 这里作者没有进行 Variable(),与之前不同\\n            # pytorch 0.4.之后tensor和Variable不再严格区分，创建的tensor就是variable\\n            # https://mp.weixin.qq.com/s?__biz=MzI0ODcxODk5OA==&mid=2247494701&idx=2&sn=ea8411d66038f172a2f553770adccbec&chksm=e99edfd4dee956c23c47c7bb97a31ee816eb3a0404466c1a57c12948d807c975053e38b18097&scene=21#wechat_redirect\\n            data = data.to(device)\\n            y = transformer_net(data)\\n            \\n            # vgg对输入的图片需要进行归一化\\n            data = normalize_batch(data)\\n            y = normalize_batch(y)\\n            feature_data = vgg(data)\\n            feature_y = vgg(y) \\n            # 疑问？？现在的feature是一个什么样子的向量？\\n            \\n            # step5.2: loss:content loss and style loss\\n            # content_loss\\n            # 在这里和书上的讲的不一样，书上是relu3_3,代码用的是relu2_2\\n            # https://blog.csdn.net/zhangxb35/article/details/72464152?utm_source=itdadao&utm_medium=referral\\n            # 均方误差指的是一个像素点的损失，可以理解N*b*h*w个元素加起来，然后除以N*b*h*w\\n            # 随机梯度下降法本身就是对batch内loss求平均后反向传播\\n            content_loss = opt.content_weight*criterion(feature_data.relu2_2,feature_y.relu2_2)\\n            # style loss\\n            # style loss:relu1_2,relu2_2,relu3_3,relu3_4 \\n            # 此时需要求每一张图片的gram矩阵\\n            \\n            style_loss = 0\\n            # tensor也可以 for i in tensor:,此时只拆解外面一层的tensor\\n            # ft_y:b*c*h*w, gm_s:1*c*h*w\\n            for ft_y, gm_s in zip(feature_y, gram_style):\\n                gram_y = gram_matrix(ft_y)\\n                style_loss += criterion(gram_y, gm_s.expand_as(gram_y))\\n            style_loss *= opt.style_weight\\n            \\n            total_loss = content_loss + style_loss\\n            optimizer.zero_grad()\\n            total_loss.backward()\\n            optimizer.step()\\n            # 获取tensor的值 tensor.item()   tensor.tolist()\\n            content_meter.add(content_loss.item())\\n            style_meter.add(style_loss.item())\\n            total_meter.add(total_loss.item())\\n            \\n            # step5.3: visualize\\n            if (ii+1)%opt.print_freq == 0 and opt.vis:\\n                # 为什么总是以这种形式进行debug\\n                if os.path.exits(opt.debug_file):\\n                    import ipdb\\n                    ipdb.set_trace()\\n                vis.plot('content_loss',content_meter.value()[0])\\n                vis.plot('style_loss',style_meter.value()[0])\\n                vis.plot('total_loss',total_meter.value()[0])\\n                # 因为现在data和y都已经经过了normalize，变成了-2~2，所以需要把它变回去0-1\\n                vis.img('input',(data.data*0.225+0.45)[0].clamp(min=0,max=1))\\n                vis.img('output',(y.data*0.225+0.45)[0].clamp(min=0,max=1))\\n            \\n        # step 5.4 save and validate and visualize\\n        if (epoch+1) % opt.save_every == 0:\\n            t.save(transformer.state_dict(), 'checkpoints/%s_style.pth' % epoch)\\n            # 保存图片的几种方法，第七章的是 \\n            # tv.utils.save_image(fix_fake_imgs,'%s/%s.png' % (opt.img_save_path, epoch),normalize=True, range=(-1,1))\\n            # vis.save竟然没找到  我的神   \\n            vis.save([opt.env])\", 'train': <function train at 0x7f04f9f13d90>, '_i4': \"def stylize(**kwargs):\\n    opt.parse(**kwargs)\\n    device = t.device('cuda') if opt.use_gpu else t.device('cpu')\\n    # data\\n    # 对单张图片进行加载验证\\n    content_image = tv.datasets.folder.default_loader(opt.content_path)\\n    # 这个transform更看不懂了，竟然不需要resize和centrocrop，这图片大小不一致，能送进去么？？？？\\n    content_transform = tv.transforms.Compose([\\n        tv.transforms.ToTensor(),\\n        tv.transforms.Lambda(lambda x: x.mul(255))\\n    ])\\n    \\n    content_image = content_transform(content_image)\\n    content_image = content_image.unsqueeze(0).to(device).detach()\\n    \\n    # model\\n    style_model = TransformerNet().eval()\\n    style_model.load_state_dict(t.load(opt.model_path, map_location=lambda _s, _: _s))\\n    style.to(device)\\n    \\n    # stylize\\n    output = style_model(content_image) # 0-255~\\n    output_data = (output.cpu().data[0]/255).clamp(min=0, max=1)\\n    tv.utils.save_image(output_data,opt.result_path)\", 'stylize': <function stylize at 0x7f04fa06ba60>, '_i5': 'def help():\\n    print(\"\"\"\\n    usage: print file.py <function> [--args=value]\\n    <function>: train | stylize\\n    example:\\n        python {0} train\\n        python {0} stylize\\n        python {0} help\\n    avaiable args :\\n    \"\"\".format(__file__))\\n    \\n    from inspect import getsource\\n    source = getsource(opt.__class__)\\n    \\n    ', 'help': <function help at 0x7f054c5dc7b8>, '_i6': \"def __name__ == '__main__'():\\n    import fire\\n    fire.Fire()\\n    \", '_i7': \"if __name__ == '__main__'():\\n    import fire\\n    fire.Fire()\\n    \", '_i8': \"if __name__ == '__main__':\\n    import fire\\n    fire.Fire()\\n    \", 'fire': <module 'fire' from '/home/tjj/anaconda3/lib/python3.6/site-packages/fire/__init__.py'>})\n",
      "\n",
      "Type:        dict\n",
      "String form: {'__name__': '__main__', '__doc__': 'Automatically created module for IPython interactive environ <...>  'fire': <module 'fire' from '/home/tjj/anaconda3/lib/python3.6/site-packages/fire/__init__.py'>}\n",
      "Length:      48\n",
      "\n",
      "Usage:       ipykernel_launcher.py \n",
      "             ipykernel_launcher.py In\n",
      "             ipykernel_launcher.py Out\n",
      "             ipykernel_launcher.py get-ipython\n",
      "             ipykernel_launcher.py exit\n",
      "             ipykernel_launcher.py quit\n",
      "             ipykernel_launcher.py Ipynb-importer\n",
      "             ipykernel_launcher.py opt\n",
      "             ipykernel_launcher.py Visualizer\n",
      "             ipykernel_launcher.py TransformerNet\n",
      "             ipykernel_launcher.py Vgg16\n",
      "             ipykernel_launcher.py get-style-data\n",
      "             ipykernel_launcher.py normalize-batch\n",
      "             ipykernel_launcher.py gram-matrix\n",
      "             ipykernel_launcher.py nn\n",
      "             ipykernel_launcher.py t\n",
      "             ipykernel_launcher.py tv\n",
      "             ipykernel_launcher.py T\n",
      "             ipykernel_launcher.py DataLoader\n",
      "             ipykernel_launcher.py meter\n",
      "             ipykernel_launcher.py tqdm\n",
      "             ipykernel_launcher.py train\n",
      "             ipykernel_launcher.py stylize\n",
      "             ipykernel_launcher.py help\n",
      "             ipykernel_launcher.py fire\n"
     ]
    },
    {
     "ename": "FireExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mFireExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tjj/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2971: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    import fire\n",
    "    fire.Fire()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
