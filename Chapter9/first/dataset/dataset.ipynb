{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这次对数据处理的方式和之前完全不一样，以前是通过这种格式来获取数据的\n",
    "```\n",
    "# 这种是自己命名一个类\n",
    "# dataset.py\n",
    "from torch.utils import data\n",
    "class DogCat(data.Dataset):\n",
    "    def __init__(self):\n",
    "    def __getitem__(self,index):\n",
    "         return data, label\n",
    "    def len(self):\n",
    "# main.py\n",
    "from data.dataset import DogCat\n",
    "dataset = DogCat()\n",
    "dataloader = DataLoader(dataset, batch_size,shuffle, num_workers)\n",
    "```\n",
    "或者是这样：\n",
    "```\n",
    "＃　这种是针对图片已经按文件夹分类放好的情况\n",
    "# main.py\n",
    "import torchvision as tv\n",
    "from torch.utils.data import DataLoader\n",
    "dataset = tv.datasets.ImageFolder(opt.data_path, transform = transforms)\n",
    "dataloader = DataLoader(dataset, batch_size,shuffle, num_workers)\n",
    "```\n",
    "\n",
    "而这次对数据的方式直接定义函数，等写完看看情况"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "函数：\n",
    "主函数：\n",
    "def get_data(): 获取数据 data,word2ix,ix2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# import Ipynb_importer\n",
    "# from config import opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import ipdb\n",
    "import re"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "get_data()----_parseRawData（）----main()---handlejson----isConstrains\n",
    "                                                   |----setenceParse\n",
    "         |----pad_sequences（）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parseRawData( src, category,constrains=None,author=None):\n",
    "\n",
    "    \"\"\"\n",
    "    code from https://github.com/justdark/pytorch-poetry-gen/blob/master/dataHandler.py\n",
    "    处理json文件，返回诗歌内容\n",
    "    \n",
    "    @param: author： 作者名字,为了限制取出的诗歌，如果为None，那么取出所有的作者\n",
    "    @param: constrains: 长度限制，比如只取五言或者七言，要求是对每句话（以逗号感叹号等分割）都必须是这个长度，不利于词\n",
    "    @param: src: json 文件存放路径\n",
    "    @param: category: 类别，有poet.song 和 poet.tang\n",
    "    @ src+category:要解析所有类型的文件\n",
    "    @return: data list\n",
    "    [\n",
    "    '床前明月光，疑是地上霜，举头望明月，低头思故乡。',\n",
    "    '一去二三里，烟村四五家，亭台六七座，八九十支花。',\n",
    "    ......\n",
    "    ]\n",
    "    所有文件的诗歌。\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    文件夹组织形式:分为两种，authors和poet；又分为两个年代，song和tang\n",
    "    \n",
    "    chinese-poetry-zhCN/poetry/authors.song.json\n",
    "    chinese-poetry-zhCN/poetry/poet.song.0.json\n",
    "    ...\n",
    "    chinese-poetry-zhCN/poetry/authors.tang.json\n",
    "    chinese-poetry-zhCN/poetry/poet.tang.0.json\n",
    "    ...\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    poet.json文件内容示例\n",
    "    data： list\n",
    "    [\n",
    "  {\n",
    "    \"strains\": [\n",
    "      \"仄仄仄仄平仄仄，平平仄平○仄仄。\", \n",
    "      \"平平仄仄平仄平，仄仄平平？仄仄。\"\n",
    "    ], \n",
    "    \"author\": \"宋太祖\", \n",
    "    \"paragraphs\": [\n",
    "      \"欲出未出光辣达，千山万山如火发。\", \n",
    "      \"须臾走向天上来，逐却残星赶却月。\"\n",
    "    ], \n",
    "    \"title\": \"日诗\"\n",
    "  }, \n",
    "  ...\n",
    "  ]\n",
    "  \n",
    "  poetry:\n",
    "  [\n",
    "  {\n",
    "    \"strains\": [\n",
    "      \"仄仄仄仄平仄仄，平平仄平○仄仄。\", \n",
    "      \"平平仄仄平仄平，仄仄平平？仄仄。\"\n",
    "    ], \n",
    "    \"author\": \"宋太祖\", \n",
    "    \"paragraphs\": [\n",
    "      \"欲出未出光辣达，千山万山如火发。\", \n",
    "      \"须臾走向天上来，逐却残星赶却月。\"\n",
    "    ], \n",
    "    \"title\": \"日诗\"\n",
    "  }\n",
    "  ]\n",
    "  \n",
    "  poem:\n",
    "  [\n",
    "      \"欲出未出光辣达，千山万山如火发。\", \n",
    "      \"须臾走向天上来，逐却残星赶却月。\"\n",
    "      ]\n",
    "  s:\n",
    "  \"欲出未出光辣达，千山万山如火发。\"\n",
    "  pdata:\n",
    "  '欲出未出光辣达，千山万山如火发。须臾走向天上来，逐却残星赶却月。'\n",
    "  sp:\n",
    "  ['欲出未出光辣达','千山万山如火发','']\n",
    "  tr: '欲出未出光辣达'或者''\n",
    "  json数据格式：\n",
    "  1.[],中间是value\n",
    "  2.{},中间是”键/值“对\n",
    "  3.可循环嵌套\n",
    "    \"\"\"\n",
    "    def sentenceParse(pdata):\n",
    "        # 按照作者的意思，这一块pdata可能会是这个样子的\n",
    "        # pdata：’-181-早嘗甘蔗淡，生摘琵琶酸。（「琵琶」，嚴壽澄校《張祜詩集》云：疑「枇杷」之誤。）好是去塵俗，煙花長一欄。‘\n",
    "        # 我观察了一两个唐诗，没有发现，不过先按照作者的来，因为对数据集也不是很清楚，也不影响后续内容\n",
    "        # 函数的目的是删除pdata内的一些不合法字符\n",
    "        \"\"\"\n",
    "        @pdata:’-181-早嘗甘蔗淡，生摘琵琶酸。（「琵琶」，嚴壽澄校《張祜詩集》云：疑「枇杷」之誤。）好是去塵俗，煙花長一欄。‘\n",
    "        @return:result:’早嘗甘蔗淡，生摘琵琶酸。好是去塵俗，煙花長一欄。‘，特殊情况下可能是空\n",
    "        \"\"\"\n",
    "        result = re.sub(u'（.*）','',pdata)\n",
    "        result = re.sub(u'{.*}','',result)\n",
    "        result = re.sub(u'《.*》','',result)\n",
    "        result = re.sub(u'[\\]\\[]','',result)\n",
    "        result = re.sub(u'[\\d-]','',result)\n",
    "        result = re.sub(u'。。',u'。',result)\n",
    "        return result\n",
    "        \n",
    "    def isConstrains(poem):\n",
    "        \"\"\"\n",
    "        poem:示例如上\n",
    "        用于判断poem中的每句话是否等于其要求的长度\n",
    "        @return： False：表示不满足，应该舍去\n",
    "                 True：表示满足，应该保留\n",
    "        \"\"\"\n",
    "        for s in poem:\n",
    "            sp = re.split(u'[，！。]',s)\n",
    "            for tr in sp:\n",
    "                if (constrains is not None) and (len(tr) != constrains) and (len(tr) != 0):\n",
    "                    return False\n",
    "        return True\n",
    "    \n",
    "    \n",
    "    \n",
    "    def handleJson(file):\n",
    "        # 对一个文件内数据进行处理\n",
    "        \"\"\"\n",
    "        @return: ['欲出未出光辣达，千山万山如火发。须臾走向天上来，逐却残星赶却月。',\n",
    "                  '欲出未出光辣达，千山万山如火发。须臾走向天上来，逐却残星赶却月。'\n",
    "                  ]\n",
    "        \"\"\"\n",
    "        rst = []\n",
    "        with open(file,'r') as f:\n",
    "            data = json.load(f) # data示例如上\n",
    "        for poetry in data:\n",
    "            if (author is not None) and (author != poetry['author']):\n",
    "                continue\n",
    "            poem = poetry[\"paragraphs\"] # list,但此时长度可能不一致\n",
    "            # 是否满足长度限制\n",
    "            if not isConstrains(poem):\n",
    "                continue\n",
    "            pdata = ''.join(poem) # '欲出未出光辣达，千山万山如火发。须臾走向天上来，逐却残星赶却月。'\n",
    "            # 删除pdata内的不合法字符\n",
    "            pdata = sentenceParse(pdata) # '欲出未出光辣达，千山万山如火发。须臾走向天上来，逐却残星赶却月。'\n",
    "            if pdata != '':\n",
    "                rst.append(pdata)\n",
    "        return rst\n",
    "        \n",
    "        \n",
    "    # main()   \n",
    "    data = []\n",
    "    # src = chinese-poetry-zhCN/poetry \n",
    "    for filename in os.listdir(src):\n",
    "        if filename.startswith(category):\n",
    "            path = os.path.join(src,filename)#path = chinese-poetry-zhCN/poetry/poet.song.0.json\n",
    "            data.extend(handleJson(path))       # 读取文件并进行解析\n",
    "            \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, maxlen=None, dtype='int32', padding='pre', truncating = 'post',value=0):\n",
    "    \"\"\"\n",
    "    code from keras\n",
    "    Pads each sequence to the same length (length of the longest sequence).\n",
    "    If maxlen is provided, any sequence longer\n",
    "    than maxlen is truncated to maxlen.\n",
    "    Truncation happens off either the beginning (default) or\n",
    "    the end of the sequence.\n",
    "    Supports post-padding and pre-padding (default).\n",
    "    Arguments:\n",
    "        sequences: list of lists where each element is a sequence\n",
    "        maxlen: int, maximum length\n",
    "        dtype: type to cast the resulting sequence.\n",
    "        padding: 'pre' or 'post', pad either before or after each sequence.\n",
    "        truncating: 'pre' or 'post', remove values from sequences larger than\n",
    "            maxlen either in the beginning or in the end of the sequence\n",
    "        value: float, value to pad the sequences to the desired value.\n",
    "    Returns:\n",
    "        x: numpy array with dimensions (number_of_sequences, maxlen)\n",
    "    Raises:\n",
    "        ValueError: in case of invalid values for `truncating` or `padding`,\n",
    "            or in case of invalid shape for a `sequences` entry.\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    @ sequences:[[1,2,3],[1,2,3,4],[1,2,3,4,5]]\n",
    "    @ return: x:[[0,1,2,3,],[1,2,3,4],[1,2,3,4]] np.array\n",
    "    由于这段代码本身作者也是参照了别人写的，里面会有一些没有意义的东西，对此，我选择基本保留作者的原意，因为这是涉及到更多的扩展性，\n",
    "    \n",
    "    \"\"\"\n",
    "    if not hasattr(sequences, '__len__'):\n",
    "        raise ValueError('`sequences` must be iterable.')\n",
    "    lengths = []\n",
    "    for x in sequences:\n",
    "        if not hasattr(x, '__len__'):\n",
    "            raise ValueError('`sequences` must be a list of iterables. '\n",
    "                             'Found non-iterable: ' + str(x))\n",
    "            lengths.append(len(x))\n",
    "    # 一共有num_samples个子列表\n",
    "    num_samples = len(sequences)\n",
    "    if maxlen is None:\n",
    "        maxlen = np.max(lengths)\n",
    "    sample_shape = tuple() # \n",
    "    # 第一个非空子列表的内有sample_shape个列表，这里我们是1.\n",
    "    # 这里的sequences其实是一个多维这个形状的:num_samples*max_len*sample_shape.并且sample_shape表明，每一个子子列表是必须可以表示成相同大小的矩阵形式的才行。\n",
    "    # 所以sample_shape其实记录的子子列表的维度,各个np的大小应该是一样的。\n",
    "    # 类似 [[np,np],[np,np],[np,np]], sample_shape = np.shape = turnc.shape[1:]\n",
    "    # 在此之所以用np.array是因为list本身只有len方法，没法返回其size，所以我们要想返回其size，可以采用np作为辅助\n",
    "    for s in sequences:\n",
    "        if len(s) >0:\n",
    "            sample_shape = np.asarray(s).shape[1:]\n",
    "            break\n",
    "    # tuple的拼接\n",
    "    x = (np.ones((num_samples, maxlen) + sample_shape) * value).astype(dtype)\n",
    "    for idx, s in enumerate(sequences):\n",
    "        if not len(s):\n",
    "            continue\n",
    "        if truncating == 'pre':\n",
    "            trunc = s[-maxlen:]\n",
    "        elif truncating == 'post':\n",
    "            trunc = s[:maxlen]\n",
    "        else:\n",
    "            raise ValueError('Truncating type \"%s\" not understood' % truncating)\n",
    "        trunc = np.asarray(trunc, dtype=dtype)\n",
    "        if trunc.shape[1:] != sample_shape:\n",
    "            raise ValueError(\n",
    "                'Shape of sample %s of sequence at position %s is different from '\n",
    "                'expected shape %s'\n",
    "                % (trunc.shape[1:], idx, sample_shape))\n",
    "        if padding == 'post':\n",
    "            x[idx,:len(trunc)] = trunc\n",
    "        elif padding == 'pre':\n",
    "            x[idx,-len(trunc):] = trunc\n",
    "        else:\n",
    "            raise ValueError('Padding type \"%s\" not understood' % padding)\n",
    "            \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(opt):\n",
    "    \"\"\"\n",
    "    @param  opt: Config对象，因为太多了，所以用opt来传入需要的参数\n",
    "    @return data: numpy二维数组，每一行是一首诗对应的字的下标 np.array([[1,2,3],[1,2,3]])\n",
    "    @return word2ix: dict 字对应序号，　类似　'月'：１\n",
    "    @return ix2word: dict 序号对应字，　类似　１：'月'\n",
    "    \n",
    "    # 编码问题值得注意一下\n",
    "    \"\"\"\n",
    "\n",
    "    # 如果已经存在处理好的数据\n",
    "    if os.path.exists(opt.pickle_path):\n",
    "        data = np.load(opt.pickle_path)\n",
    "        data, word2ix, ix2word = data['data'], data['word2ix'], data['ix2word']\n",
    "        return data, word2ix, ix2word\n",
    "    # 如果没有处理好的数据\n",
    "    \n",
    "    # 处理原始的json文件，将全部的poem取出来，形成list\n",
    "    # data: [\n",
    "    # '床前明月光，疑是地上霜，举头望明月，低头思故乡。',\n",
    "    # '一去二三里，烟村四五家，亭台六七座，八九十支花。'\n",
    "    # ...]\n",
    "    data = _parseRawData( opt.data_path, opt.category, opt.constrains, opt.author) #　读取文件夹下的所有文件并进行解析成句子\n",
    "    # 接下来要形成word2ix和ix2word\n",
    "    words = {_word for _sentence in data for _word in _sentence}\n",
    "    word2ix = {_word:_ix for _ix,_word in enumerate(words)} # dict\n",
    "    word2ix['<EOP>'] = len(word2ix)\n",
    "    word2ix['<START>'] = len(word2ix)\n",
    "    word2ix['</s>'] = len(word2ix)\n",
    "    ix2word = {_ix:_word for _word,_ix in list(word2ix.items())}\n",
    "    # 将诗歌转化为数字\n",
    "    # 接下来把data想要写成这样子的\n",
    "    # [['<START>','床', '前', '明', '月', '光', '，', '疑', '是', '地', '上', '霜','<EOP>'],\n",
    "    # ['<START>','床', '前', '明', '月', '光', '，', '疑', '是', '地', '上', '霜','<EOP>'],\n",
    "    # ...]\n",
    "    # data = [ ''.join(['<START>',_data,'<EOP>']) for _data in data]\n",
    "    data = [ ['<START>']+list(_data)+['<EOP>'] for _data in data]\n",
    "    # data: [\n",
    "    # '床前明月光，疑是地上霜，举头望明月，低头思故乡。',\n",
    "    # '一去二三里，烟村四五家，亭台六七座，八九十支花。'\n",
    "    # ...]\n",
    "    sequences = [ [word2ix[_word]  for _word in _data ]for _data in data]\n",
    "    # sequences:[[1,2,3],[1,2,3],[1,2,3]]\n",
    "    # 对诗歌进行增删成固定长度的诗歌\n",
    "    data = pad_sequences(sequences, \n",
    "                         maxlen=opt.maxlen, \n",
    "                         dtype='int32', \n",
    "                         padding='pre', \n",
    "                         truncating = 'post',\n",
    "                         value=len(word2ix)-1)\n",
    "    # pad_data:[[0,1,2,3,],[1,2,3,4],[1,2,3,4]]\n",
    "    # 保存成文件\n",
    "    word2ix = np.array(word2ix)\n",
    "    ix2word = np.array(ix2word)\n",
    "    np.savez_compressed(opt.pickle_path, \n",
    "                        data=data, word2ix = word2ix, ix2word = ix2word)\n",
    "    \n",
    "    return data, word2ix, ix2word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
