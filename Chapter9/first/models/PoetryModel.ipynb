{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embedding需要重新看看说明，是一个什么样的原理\n",
    "\n",
    "LSTM的理解：(https://www.zybuluo.com/hanbingtao/note/581764)\n",
    "\n",
    "LSTM的理解：colah.github.io\n",
    "\n",
    "晚上去找了同学，才知道了一点关于LSTM的编码器和解码器，不过对于loss的定义还是不清晰，这里还是等写完main函数再看看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 2, 30])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "m = nn.Linear(20,30)\n",
    "m.bias.shape\n",
    "input = torch.randn(1,3,2,20)\n",
    "input.shape\n",
    "output = m(input)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoetryModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers):\n",
    "        \"\"\"\n",
    "        @vocab_size: int 表示输入字的个数\n",
    "        @embedding_dim: int 表示希望这些字可以被多少维的向量表示\n",
    "        这两个参数的用法  nn.Embedding(vocab_size, embedding_dim)\n",
    "        @hidden_dim:  int 是LSTM的隐藏单元的维度\n",
    "        @return: \n",
    "        \"\"\"\n",
    "        super(PoetryModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # 这个lstm的定义有疑问\n",
    "        self.lstm = nn.LSTM(embedding_dim, self.hidden_dim, num_layers)\n",
    "        # Linear层还有异议，需要重新看一下定义\n",
    "        self.linear1 = nn.Linear(self.hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, input, hidden=None):\n",
    "        \"\"\"\n",
    "        @input: tensor, size： (seq_len, batch_size)  \n",
    "        @hidden: tuple,(tensor, tensor) 是LSTM模型的初始输入\n",
    "        \"\"\"\n",
    "        seq_len, batch_size = input.size()\n",
    "        \n",
    "        if hidden is None:\n",
    "            # 这里的new方法存疑,已经搞清楚了，就相当于生成同类型的tensor\n",
    "            # view()和reshape的功能差不多，前者只能是连续内存，后者都行， \n",
    "            # view_as(t.tensor(4,3))\n",
    "            h_0 = input.data.new(2, batch_size, self.hidden_dim).fill_(0).float()\n",
    "            c_0 = input.data.new(2, batch_size, self.hidden_dim).fill_(0).float()\n",
    "        else:\n",
    "            h_0, c_0 = hidden\n",
    "        \n",
    "        embeds = self.embeddings(input) # embeds size: (seq_len, batch_size, embeding_dim)\n",
    "        output, hidden = self.lstm(embeds, (h_0, c_)) \n",
    "        # output size (seq_len, batch_size, hidden_dim)\n",
    "        # hidden tuple,每一个元素的维度 (num_layers, batch_size, hidden_dim)\n",
    "        # 这里感觉上下两个维都对不上啊，感觉\n",
    "        # 这里维度能对应上了， Linear层的输入是(batch_size,*,in_features) 输出是（batch_size, *, out_features)\n",
    "        output = self.linear1(output.view(-1, self.hidden_dim))\n",
    "        # output size (seq_len*batch_size, vocab_size)\n",
    "        # hidden tuple,每一个元素的维度 (num_layers, batch_size, hidden_dim)\n",
    "        return output, hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
